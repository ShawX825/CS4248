
#!pip3 install unidecode
!pwd
import os
import numpy as np
from numpy import random as np_random
#import random
import copy
import itertools
from os import listdir
from os.path import isfile, join
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import TextTilingTokenizer
import shutil
import json
import re
from transformers import BertTokenizer
import torch
from keras.preprocessing.sequence import pad_sequences
from transformers import BertForNextSentencePrediction
import statistics
from sklearn.metrics import mean_absolute_error, f1_score
from unidecode import unidecode
import nltk

# avodi utf-8 encoding issue
def encode_label(label):
    for i in range(len(label)):
        for key in label[i]:
            if isinstance(label[i][key], str):
                label[i][key] = unidecode(label[i][key])
            elif isinstance(label[i][key], int):
                continue
            else:
                for j in range(len(label[i][key])):
                    if isinstance(label[i][key][j], str):
                        label[i][key][j] = unidecode(label[i][key][j])
                    elif isinstance(label[i][key][j], int):
                        continue
    return label

def dump_data(data, filename):
    !pwd
    out_file = open(filename, "w")
    json.dump(data, out_file, indent=2)
    out_file.close()


path_input_docs = 'gdrive/My Drive/CS4248project/TUCORE-GCN/datasets/DialogRE'

input_files = ['dev.json', 'train.json','test.json']#[f for f in listdir(path_input_docs) if isfile(join(path_input_docs, f) and f.endswith(".json"))]


c = 0
pick_num = 3
score_wd = 0; score_mae = 0; score_f1 = 0; score_pk = 0;
dp_var = []


# for file in dev, valid, train 
for i in range(len(input_files)):
	file = input_files[i]
	processed_data = []
	max_topic = 0
	data = json.load(open(path_input_docs + "/" + file))
	count = 0
	for sample in data:
		count += 1
		if count % 50 == 0:
			print("processed " + str(count))
		text = []
		# dialogue that has no speaker id
		# format: xxxxxx
		sample_text = [unidecode(sentence) for sentence in sample[0]]
		for sentence in sample_text:
			speaker = ""
			try:
				speaker = re.search(r"(Speaker\s\d+\s*,\s)*(Speaker\s\d+\s*:\s)", sentence).group(-1)
			except:
				pass
			###
			content = re.sub(r"(Speaker\s\d+\s*,\s)*(Speaker\s\d+\s*:\s)","", sentence)
			# split one speaker's multiple lines into seperate turns
			###
			#contents = nltk.tokenize.sent_tokenize(content)
			text += [content]
			
		#text = "\n\n".join(text)
		
		topic_labels = np.zeros(len(text))
		text = "\n\n".join(text)
  	
		tt = TextTilingTokenizer(smoothing_width=4)
		try:
			paragraphs = tt.tokenize(text)
      
			base = 0
			for i in paragraphs:
				count = len(i.split("\n\n"))
				#print("count ----- : ", count)
				base += count
				topic_labels[base:] += 1
			
		except:
			pass
     
		max_topic = max(max_topic, topic_labels[-1])
		#print(topic_labels)
		
		#print("vector size: " + str(len(paragraphs)))
		
		result_text = []
		result_text.append(sample_text)
		result_text.append(encode_label(sample[1]))
		result_text.append(topic_labels.tolist())
		processed_data.append(result_text)
		#print("saving data")
	print("done " + file)
	print("max_topic: " + str(max_topic))	
	dump_data(processed_data, path_input_docs + "/processed_" + file)#file)
		
